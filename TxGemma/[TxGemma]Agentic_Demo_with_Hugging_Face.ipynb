{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fgVWTMK9SNz"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Agentic-Tx Demo with Hugging Face\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/TxGemma/[TxGemma]Agentic_Demo_with_Hugging_Face.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/TxGemma/%5BTxGemma%5DAgentic_Demo_with_Hugging_Face.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/collections/google/txgemma-release-67dd92e931c857d15e4d1e87\">\n",
        "      <img alt=\"HuggingFace logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on HuggingFace\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "This Colab notebook provides a basic demo of using Agentic-Tx, a therapeutics-focused LLM agent. Agentic-Tx builds on TxGemma, a collection of large language models built upon Gemma 2, that generates predictions, classifications or text based on therapeutic related data.\n",
        "\n",
        "Learn more about TxGemma at [this page](https://developers.google.com/health-ai-developer-foundations/txgemma)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xt2XZgaaH2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the TxGemma model. Choose an appropriate runtime when starting your Colab session.\n",
        "\n",
        "You can try out TxGemma 2B and 9B* for free using a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **▾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "*To run the demo with both TxGemma 2B predict and 9B chat models on a T4 GPU, use 4-bit quantization to reduce memory usage and speed up inference. Note that the performance of quantized versions has not been evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ITcQtdal7J"
      },
      "source": [
        "### Get access to TxGemma\n",
        "\n",
        "Before you get started, make sure that you have access to TxGemma models on Hugging Face:\n",
        "\n",
        "1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).\n",
        "2. Head over to the [TxGemma model page](https://huggingface.co/google/txgemma-2b-predict) and accept the usage conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFQnPL2a9Dj"
      },
      "source": [
        "### Configure your HF token and Gemini token\n",
        "\n",
        "Add your Hugging Face & Gemini token to the Colab Secrets manager to securely store it.\n",
        "\n",
        "1. Ensure you have a [Gemini API key](https://ai.google.dev/gemini-api/docs/api-key) and a [HF_Token](https://huggingface.co/docs/hub/en/security-tokens)\n",
        "2. Open your Google Colab notebook and click on the 🔑 Secrets tab in the left panel. <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
        "\n",
        "3. Create two new secrets with the name `HF_TOKEN` and `GEMINI_API_KEY`.\n",
        "4. Copy/paste your token key into the Value input box of `HF_TOKEN` and `GEMINI_API_KEY` .\n",
        "5. Toggle the button on the left to allow notebook access to the secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SJRgvl-Wh_VM"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "genai.configure(api_key=userdata.get(\"GEMINI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qocWBSYmb0MA"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyPXnIjML6go",
        "outputId": "a9e66093-3c14-4511-d5bf-d1a3917fc8c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.5/664.8 MB\u001b[0m \u001b[31m160.3 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade --quiet accelerate bitsandbytes huggingface_hub transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z8YZ8pT1QD5"
      },
      "source": [
        "### Load prompt template\n",
        "\n",
        "First, load a JSON file that contains the prompt format for various TDC tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUhpMxJq1yNi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "tdc_prompts_filepath = hf_hub_download(\n",
        "    repo_id=\"google/txgemma-2b-predict\",\n",
        "    filename=\"tdc_prompts.json\",\n",
        ")\n",
        "\n",
        "with open(tdc_prompts_filepath, \"r\") as f:\n",
        "    tdc_prompts_json = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KUYpBH1cZA1"
      },
      "source": [
        "### Download the prediction and chat model from Hugging Face Hub\n",
        "\n",
        "Here, we are going on HuggingFace to download and load both the prediction and chat version of TxGemma. These will later be transformed into tools.\n",
        "\n",
        "You can select the variants to use for Agentic-Tx at the dropdown and whether you want to include a chat variant. Make sure your runtime has enough RAM to run all the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTNyftcqF3YO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "PREDICT_VARIANT = \"2b-predict\"  # @param [\"2b-predict\", \"9b-predict\", \"27b-predict\"]\n",
        "CHAT_VARIANT = \"9b-chat\" # @param [\"9b-chat\", \"27b-chat\"]\n",
        "USE_CHAT = True # @param {type: \"boolean\"}\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "predict_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{PREDICT_VARIANT}\")\n",
        "predict_model = AutoModelForCausalLM.from_pretrained(\n",
        "    f\"google/txgemma-{PREDICT_VARIANT}\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "\n",
        "if USE_CHAT:\n",
        "    chat_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{CHAT_VARIANT}\")\n",
        "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "        f\"google/txgemma-{CHAT_VARIANT}\",\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBmbHe2Hg4Y_"
      },
      "source": [
        "### Run inference on a sample binary classification task\n",
        "\n",
        "Let's first try making predictions with both the models to make sure they are working. We're just going to use a sample TDC task with a sample drug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgEN-mgbO6Gm"
      },
      "outputs": [],
      "source": [
        "## Example task and input\n",
        "task_name = \"BBB_Martins\"\n",
        "input_type = \"{Drug SMILES}\"\n",
        "drug_smiles = \"CN1C(=O)CN=C(C2=CCCCC2)c2cc(Cl)ccc21\"\n",
        "TDC_PROMPT = tdc_prompts_json[task_name].replace(input_type, drug_smiles)\n",
        "\n",
        "def txgemma_predict(prompt):\n",
        "    input_ids = predict_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = predict_model.generate(**input_ids, max_new_tokens=8)\n",
        "    return predict_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def txgemma_chat(prompt):\n",
        "    input_ids = chat_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = chat_model.generate(**input_ids, max_new_tokens=32)\n",
        "    return chat_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Prediction model response: {txgemma_predict(TDC_PROMPT)}\")\n",
        "if USE_CHAT: print(f\"Chat model response: {txgemma_chat(TDC_PROMPT)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLh_zF2oe2QT"
      },
      "source": [
        "### Making our first TxGemma tool (Chat)\n",
        "We are now going to make a tool for our agent to use: a chat interface for our Gemini-based Agentic-Tx and TxGemma-Chat. This tool will allow our Agentic-Tx to ask TxGemma therapeutically relevant questions. We're going to provide some functionality to check if the tool was used (tool_is_used)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE0s6u5ge8oz"
      },
      "outputs": [],
      "source": [
        "# This will allow us to extract content from inside of ticks\n",
        "def extract_prompt(text, word):\n",
        "    code_block_pattern = rf\"```{word}(.*?)```\"\n",
        "    code_blocks = re.findall(code_block_pattern, text, re.DOTALL)\n",
        "    extracted_code = \"\\n\".join(code_blocks).strip()\n",
        "    return extracted_code\n",
        "\n",
        "# This class will allow us to inferface with TxGemma\n",
        "class TxGemmaChatTool:\n",
        "    def __init__(self):\n",
        "      self.tool_name = \"Chat Tool\"\n",
        "\n",
        "    def use_tool(self, question):\n",
        "        # Here, we are submitting a question to TxGemma\n",
        "        response = txgemma_chat(question)\n",
        "        return response\n",
        "\n",
        "    def tool_is_used(self, query):\n",
        "        # This just checks to see if the tool call was evoked\n",
        "        return \"```TxGemmaChat\" in query\n",
        "\n",
        "    def process_query(self, query):\n",
        "        # Here, we clean to query to remove the tool call\n",
        "        return extract_prompt(query, word=\"TxGemmaChat\")\n",
        "\n",
        "    def instructions(self):\n",
        "        # Here, we are **very** descriptively explaining how the tool works to the agent\n",
        "        # This will be useful later on\n",
        "        return (\n",
        "            \"=== Therapeutic Chat Tool Instructions ===\\n\"\n",
        "            \"### What This Tool Does\\n\"\n",
        "            \"The Therapeutic Chat Tool allows you to chat with a knowledgeable large language model named TxGemma trained on many therapeutics datasets.\"\n",
        "            \"### When and Why You Should Use It\\n\"\n",
        "            \"- If you have therapeutics related questions that you would benefit from asking TxGemma from.\\n\"\n",
        "            \"### How to Use It\\n\"\n",
        "            \"Format your query with triple backticks (```), and start with `TxGemmaChat`. Then on a new line:\\n\"\n",
        "            \"1) **Any question you would like to ask**\\n\\n\"\n",
        "            \"Example:\\n\"\n",
        "            \"```TxGemmaChat\\n\"\n",
        "            \"What is a common drug used to treat ovarian cancer?\\n\"\n",
        "            \"```\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WIH0n7gfRyU"
      },
      "source": [
        "Let's now try out the tool by asking TxGemma-Chat a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqiAWzFSfWgF"
      },
      "outputs": [],
      "source": [
        "if USE_CHAT:\n",
        "    chat_tool = TxGemmaChatTool()\n",
        "    response = chat_tool.use_tool(\"Can Aspirin help with headaches? Yes or no?\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQsHhOiAfW0-"
      },
      "source": [
        "### Making a TxGemma prediction (Clinical Toxicity)\n",
        "We are now going to make a tool for our agent that allows them to predict whether a drug will be toxic in clinical trials. For this, we will interface with the predict version of TxGemma, which has strong predictive capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzP6UFaefXI9"
      },
      "outputs": [],
      "source": [
        "# This class will allow us to predict toxicity using TxGemma\n",
        "class ClinicalTox:\n",
        "    def __init__(self):\n",
        "      self.tool_name = \"Clinical Toxicity Prediction\"\n",
        "\n",
        "    def use_tool(self, smiles_string):\n",
        "        # Here, we are submitting the smiles to TxGemma, and returning the response\n",
        "        prediction = txgemma_predict(tdc_prompts_json[\"ClinTox\"].replace(\"{Drug SMILES}\", smiles_string))\n",
        "        if \"(A)\" in prediction:   prediction = f\"{smiles_string} is not toxic!\"\n",
        "        elif \"(B)\" in prediction: prediction = f\"{smiles_string} is toxic!\"\n",
        "        return prediction\n",
        "\n",
        "    def tool_is_used(self, query):\n",
        "        # This just checks to see if the tool call was evoked\n",
        "        return \"```ClinicalToxTool\" in query\n",
        "\n",
        "    def process_query(self, query):\n",
        "        # Here, we clean to query to remove the tool call\n",
        "        return extract_prompt(query, word=\"ClinicalToxTool\")\n",
        "\n",
        "    def instructions(self):\n",
        "        # Here, we are explaining how the tool works to the agent\n",
        "        return (\n",
        "            \"=== Clinical Toxicity Instructions ===\\n\"\n",
        "            \"The Clinical Toxicity Tool is designed to predict potential for toxicity for humans in clinicial trials.\\n\"\n",
        "            \"You can test the toxicity of different SMILES strings as they might affect humans.\\n\"\n",
        "            \"To properly use this tool, follow the format outlined below:\\n\"\n",
        "            \"1. **Form a clinical toxicity query**:\\n\"\n",
        "            \"```ClinicalToxTool\\n<SMILES string here>\\n```\\n\"\n",
        "            \"Example: ```ClinicalToxTool\\nCN(C)C(=N)N=C(N)N\\n```\\n\"\n",
        "            \"- Replace `<SMILES string here>` with an exact smiles string. \"\n",
        "            \"A results will be returned to you describing the clinical toxicity.\\n\"\n",
        "            \"**Important Formatting Details**:\\n\"\n",
        "            \"- Use `ClinicalToxTool` as the exact keyword to begin your query.\\n\"\n",
        "            \"- Place your text after `ClinicalToxTool` on a new line.\\n\"\n",
        "            \"- Enclose the entire query using three backticks (```), as shown in the example above.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcxJSeOdiwb_"
      },
      "source": [
        "Now we can test this out. The drug CC(=O)OC1=CC=CC=C1C(=O)O is a well known non-toxic drug, Aspirin. Let's test out the ClinicalTox to make sure the tool works correctly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trTNftLCix7L"
      },
      "outputs": [],
      "source": [
        "# This is an example of gemini using the tool to predict toxicity\n",
        "clintox = ClinicalTox()\n",
        "prediction_aspirin = clintox.use_tool(\"CC(=O)OC1=CC=CC=C1C(=O)O\")\n",
        "print(prediction_aspirin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qZp8hTfizAy"
      },
      "source": [
        "### Making a PubMed article search tool\n",
        "We are now going to make a tool for our agent that allows them to search through PubMed articles. This allows our agent to have access to the latest biomedical research. We can access PubMed through the biopython API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwfONVb31dUf"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXP04E2MizVY"
      },
      "outputs": [],
      "source": [
        "from Bio import Medline, Entrez\n",
        "\n",
        "# This class will allow us to interface with PubMed\n",
        "class PubMedSearch:\n",
        "    def __init__(self):\n",
        "      self.tool_name = \"PubMed Search\"\n",
        "\n",
        "    def tool_is_used(self, query: str):\n",
        "        # This just checks to see if the tool call was evoked\n",
        "        return \"```PubMedSearch\" in query\n",
        "\n",
        "    def process_query(self, query: str):\n",
        "        # Here, we clean to query to remove the tool call\n",
        "        search_text = extract_prompt(query, word=\"PubMedSearch\")\n",
        "        return search_text.strip()\n",
        "\n",
        "    def use_tool(self, search_text):\n",
        "        # Here, we are searching through PubMed and returning relevant articles\n",
        "        pmids = list()\n",
        "        handle = Entrez.esearch(db=\"pubmed\", sort=\"relevance\", term=search_text, retmax=3)\n",
        "        record = Entrez.read(handle)\n",
        "        pmids = record.get(\"IdList\", [])\n",
        "        handle.close()\n",
        "\n",
        "        if not pmids:\n",
        "            return f\"No PubMed articles found for '{search_text}' Please try a simpler search query.\"\n",
        "\n",
        "        fetch_handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pmids), rettype=\"medline\", retmode=\"text\")\n",
        "        records = list(Medline.parse(fetch_handle))\n",
        "        fetch_handle.close()\n",
        "\n",
        "        result_str = f\"=== PubMed Search Results for: '{search_text}' ===\\n\"\n",
        "        for i, record in enumerate(records, start=1):\n",
        "            pmid = record.get(\"PMID\", \"N/A\")\n",
        "            title = record.get(\"TI\", \"No title available\")\n",
        "            abstract = record.get(\"AB\", \"No abstract available\")\n",
        "            journal = record.get(\"JT\", \"No journal info\")\n",
        "            pub_date = record.get(\"DP\", \"No date info\")\n",
        "            authors = record.get(\"AU\", [])\n",
        "            authors_str = \", \".join(authors[:3])\n",
        "            result_str += (\n",
        "                f\"\\n--- Article #{i} ---\\n\"\n",
        "                f\"PMID: {pmid}\\n\"\n",
        "                f\"Title: {title}\\n\"\n",
        "                f\"Authors: {authors_str}\\n\"\n",
        "                f\"Journal: {journal}\\n\"\n",
        "                f\"Publication Date: {pub_date}\\n\"\n",
        "                f\"Abstract: {abstract}\\n\")\n",
        "        return f\"Query: {search_text}\\nResults: {result_str}\"\n",
        "\n",
        "    def instructions(self):\n",
        "        # Here, we are explaining how the tool works to the agent\n",
        "        return (\n",
        "            f\"{'@' * 10}\\n@@@ PubMed Search Tool Instructions @@@\\n\\n\"\n",
        "            \"### What This Tool Does\\n\"\n",
        "            \"The PubMed Search Tool queries the NCBI Entrez API (PubMed) for a given search phrase, \"\n",
        "            \"and retrieves metadata for a few of the top articles (PMID, title, authors, journal, date, abstract).\\n\\n\"\n",
        "            \"### When / Why You Should Use It\\n\"\n",
        "            \"- To find **scientific literature** references on a specific biomedical topic.\\n\"\n",
        "            \"- To retrieve **abstracts, titles, authors**, and other metadata.\\n\\n\"\n",
        "            \"### Query Format\\n\"\n",
        "            \"Wrap your request with triple backticks, starting with `PubMedSearch`. For example:\\n\\n\"\n",
        "            \"```PubMedSearch\\ncancer immunotherapy\\n```\\n\\n\"\n",
        "            \"### Example\\n\"\n",
        "            \"```PubMedSearch\\nmachine learning in drug discovery\\n```\\n\"\n",
        "            \"- This will search PubMed for articles related to 'machine learning in drug discovery', \"\n",
        "            \"fetch up to 3 PMIDs, and return their titles, abstracts, etc.\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl23xROHkRI_"
      },
      "source": [
        "Let's now see how this tool works by getting the most up-to-date knowledge on drugs used for ovarian cancer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMWEiM29kQ4o"
      },
      "outputs": [],
      "source": [
        "pubmed_tool = PubMedSearch()\n",
        "search_results = pubmed_tool.use_tool(\"Drugs used for ovarian cancer\")\n",
        "print(search_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXejx5atkSdX"
      },
      "source": [
        "## Wrapping it all together\n",
        "\n",
        "### Creating a tool manager\n",
        "Now we need to package all of this together in a way that allows our agent to use these tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UTRDs7wkS4V"
      },
      "outputs": [],
      "source": [
        "# The tool manager will hold all of the tools, and provide an interface for the agent\n",
        "class ToolManager:\n",
        "    def __init__(self, toolset):\n",
        "        self.toolset = toolset\n",
        "\n",
        "    def tool_prompt(self):\n",
        "        # This will let the agent know what tools it has access to\n",
        "        tool_names = \", \".join([tool.tool_name for tool in self.toolset])\n",
        "        return f\"You have access to the following tools: {tool_names}\\n{self.tool_instructions()}. You can only use one tool at a time. These are the only tools you have access to nothing else.\"\n",
        "\n",
        "    def tool_instructions(self):\n",
        "        # This allows the agent to know how to use the tools\n",
        "        tool_instr = \"\\n\".join([tool.instructions() for tool in self.toolset])\n",
        "        return f\"The following is a set of instructions on how to use each tool.\\n{tool_instr}\"\n",
        "\n",
        "    def use_tool(self, query):\n",
        "        # This will iterate through all of the tools\n",
        "        # and find the correct tool that the agent requested\n",
        "        for tool in self.toolset:\n",
        "            if tool.tool_is_used(query):\n",
        "                # use the tool and return the output\n",
        "                return tool.use_tool(tool.process_query(query))\n",
        "        return f\"No tool match for search: {query}\"\n",
        "\n",
        "if USE_CHAT:\n",
        "    tools = ToolManager([TxGemmaChatTool(), ClinicalTox(), PubMedSearch()])\n",
        "else:\n",
        "    tools = ToolManager([ClinicalTox(), PubMedSearch()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgIxiRppsxL3"
      },
      "source": [
        "### Creating a Gemini inference tool\n",
        "The following tool will allow us to inference Gemini, which we can use to act as the agent orchestrator. Gemini will pick from the different tools we implemented as well as provide input to those tools in order to solve a problem. Let's make the inference structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6LXiWUPsxiD"
      },
      "outputs": [],
      "source": [
        "def inference_gemini(prompt, system_prompt, model_str):\n",
        "  # Check to see that our model string matches\n",
        "  if model_str == \"gemini-2.0-pro\":\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-2.0-pro-exp-02-05\", system_instruction=system_prompt)\n",
        "    response = model.generate_content(prompt)\n",
        "    answer = response.text\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ZltV6-CZB-"
      },
      "source": [
        "## Creating a therapeutics agent\n",
        "\n",
        "Finally, we are going to create a Agentic-Tx with access to the three tools we made here, a therapeutics agent. We are going to ask it which drug is preferable for further development: a preferable drug, CC(=O)OC1=CC=CC=C1C(=O)O, versus a non-preferable drug, O=C(CCCCCCC(=O)Nc1ccccc1)NO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSvtrelYCZYh"
      },
      "outputs": [],
      "source": [
        "# This class defines our Agentic-Tx, wrapping together all of our tools and the orchestrator\n",
        "class AgenticTx:\n",
        "  def __init__(self, tool_manager, model_str, num_steps=5):\n",
        "    self.curr_steps = 0\n",
        "    self.num_steps = num_steps\n",
        "    self.model_str = model_str\n",
        "    self.tool_manager = tool_manager\n",
        "    self.thoughts = list()\n",
        "    self.actions  = list()\n",
        "    self.observations = list()\n",
        "\n",
        "  def reset(self):\n",
        "    # Reset the number of steps taken\n",
        "    self.curr_steps = 0\n",
        "\n",
        "  def system_prompt(self, use_tools=True):\n",
        "    # These are the system instructions for AgenticTx\n",
        "    role_prompt = \"You are an expert therapeutic agent. You answer accurately and thoroughly.\"\n",
        "    prev_actions = f\"You can perform a maximum of {self.num_steps} actions. You have performed {self.curr_steps} and have {self.num_steps - self.curr_steps - 1} left.\"\n",
        "    if use_tools: tool_prompt = \"You can use tools to solve problems and answer questions. \" + self.tool_manager.tool_prompt()\n",
        "    else: tool_prompt = \"You cannot use any tools right now.\"\n",
        "    return f\"{role_prompt} {prev_actions} {tool_prompt}\"\n",
        "\n",
        "  def prior_information(self, query):\n",
        "      info_txt = f\"Question: {query}\\n\" if query is not None else \"\"\n",
        "      for _i in range(self.curr_steps):\n",
        "          info_txt += f\"### Thought {_i + 1}: {self.thoughts[_i]}\\n\"\n",
        "          info_txt += f\"### Action {_i + 1}: {self.actions[_i]}\\n\"\n",
        "          info_txt += f\"### Observation {_i + 1}: {self.observations[_i]}\\n\\n\"\n",
        "          info_txt += \"@\"*20\n",
        "      return info_txt\n",
        "\n",
        "  def step(self, question):\n",
        "    for _i in range(self.num_steps):\n",
        "      if self.curr_steps == self.num_steps-1:\n",
        "        return inference_gemini(\n",
        "            model_str=self.model_str,\n",
        "            prompt=f\"{self.prior_information(question)}\\nYou must now provide an answer to this question {question}\",\n",
        "            system_prompt=self.system_prompt(use_tools=False))\n",
        "      else:\n",
        "        # Provide a thought step, planning for the model\n",
        "        thought = inference_gemini(\n",
        "            model_str=self.model_str,\n",
        "            prompt=f\"{self.prior_information(question)}\\nYou cannot currently use tools but you can think about the problem and what tools you want to use. This was the question, think about plans for how to use tools to answer this {question}. Let's think step by step (respond with only 1-2 sentences).\\nThought: \",\n",
        "            system_prompt=self.system_prompt(use_tools=False))\n",
        "        # Provide a took action for the model\n",
        "        action = inference_gemini(\n",
        "            model_str=self.model_str,\n",
        "            prompt=f\"{self.prior_information(question)}\\n{thought}\\nNow you must use tools to answer the following user query [{question}], closely following the tool instructions. Tool\",\n",
        "            system_prompt=self.system_prompt(use_tools=True))\n",
        "        obs = self.tool_manager.use_tool(action)\n",
        "\n",
        "        print(\"Thought:\", thought)\n",
        "        print(\"Action:\",  action)\n",
        "        print(\"Observation:\",  obs)\n",
        "\n",
        "        self.thoughts.append(thought)\n",
        "        self.actions.append(action)\n",
        "        self.observations.append(obs)\n",
        "\n",
        "        self.curr_steps += 1\n",
        "\n",
        "\n",
        "agentictx = AgenticTx(tool_manager=tools, model_str=\"gemini-2.0-pro\")\n",
        "# The model should select CC(=O)OC1=CC=CC=C1C(=O)O because O=C(CCCCCCC(=O)Nc1ccccc1)NO is toxic\n",
        "response = agentictx.step(\"Which of the following drugs is preferred for further development? 1. CC(=O)OC1=CC=CC=C1C(=O)O or 2. O=C(CCCCCCC(=O)Nc1ccccc1)NO\")\n",
        "print(\"\\nFinal Response:\", response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[TxGemma]Agentic_Demo_with_Hugging_Face.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}